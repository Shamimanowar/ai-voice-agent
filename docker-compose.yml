services:
  backend:
    build: ./backend
    volumes:
      - ./backend:/app
    ports:
      - "8000:8000"
    depends_on:
      - llama
    environment:
      - PYTHONUNBUFFERED=1
  llama:
    build:
      context: ./llama.cpp
      dockerfile: Dockerfile
    command: ["/app/build/bin/llama-server", "-m", "/models/llama-2-7b-chat.gguf", "--host", "0.0.0.0", "--port", "8001"]
    ports:
      - "8001:8001"
    volumes:
      - ./models:/models
    environment:
      - MODEL=/models/llama-2-7b-chat.gguf
  frontend:
    build: ./frontend
    ports:
      - "8080:80"
    depends_on:
      - backend
    volumes:
      - ./frontend:/app
volumes:
  backend_data:
